{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T07:39:09.546454Z",
     "start_time": "2025-01-14T07:39:09.542106Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db548dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress UserWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30edfcad6e33d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T07:39:58.455172Z",
     "start_time": "2025-01-14T07:39:55.143157Z"
    }
   },
   "outputs": [],
   "source": [
    "df_trips = pd.read_csv('trips/trips2015_1.5M.csv.zip')\n",
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f809afb",
   "metadata": {},
   "source": [
    "# Memory usage optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd331850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84dd9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce memory usage, we will limit the datatypes of the columns\n",
    "df_trips[['VendorID', 'passenger_count', 'RateCodeID', 'payment_type']] = df_trips[['VendorID', 'passenger_count', 'RateCodeID', 'payment_type']].astype('int8')\n",
    "df_trips[['fare_amount','dropoff_longitude', 'dropoff_latitude', 'pickup_longitude', 'pickup_latitude', 'extra', 'mta_tax', 'tip_amount','tolls_amount' ]] = df_trips[['fare_amount','dropoff_longitude', 'dropoff_latitude', 'pickup_longitude', 'pickup_latitude', 'extra', 'mta_tax', 'tip_amount','tolls_amount']].astype('float16')\n",
    "df_trips['total_amount'] = df_trips['total_amount'].astype('float32')\n",
    "df_trips['trip_distance'] = df_trips['trip_distance'].astype('float16')\n",
    "df_trips.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f42966",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0725d594",
   "metadata": {},
   "source": [
    "## Adding the trip distance length in time units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb345a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the duration cost matrix later, we will need to use the trip distances in time units, which we currently do not have\n",
    "# We can do this based on the two datetime columns\n",
    "\n",
    "# First, turning the two columns into datetime objects\n",
    "df_trips[\"tpep_pickup_datetime\"] = pd.to_datetime(df_trips[\"tpep_pickup_datetime\"])\n",
    "df_trips[\"tpep_dropoff_datetime\"] = pd.to_datetime(df_trips[\"tpep_dropoff_datetime\"])\n",
    "\n",
    "# Then, we can calculate the time length of the trip\n",
    "df_trips['time_length'] = (df_trips[\"tpep_dropoff_datetime\"] - df_trips[\"tpep_pickup_datetime\"]).dt.total_seconds() / 3600\n",
    "df_trips = df_trips.drop(columns=['tpep_dropoff_datetime', 'tpep_pickup_datetime', 'store_and_fwd_flag'])\n",
    "\n",
    "df_trips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23fd35",
   "metadata": {},
   "source": [
    "## Limiting the variables' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clean the data, we will remove rows with missing values and outliers by filtering the data on specific conditions (explained in the report)\n",
    "print(f\"Dataframe shape before cleaning: {df_trips.shape}\")\n",
    "\n",
    "df_trips_filtered = df_trips.loc[\n",
    "    (df_trips['trip_distance'] < 21.0) & \n",
    "    (df_trips['trip_distance'] > 0.2) & \n",
    "    (df_trips['dropoff_longitude'] <= -73) & \n",
    "    (df_trips['dropoff_longitude'] > -75) & \n",
    "    (df_trips['dropoff_latitude'] >= 40) & \n",
    "    (df_trips['dropoff_latitude'] < 42) &\n",
    "    (df_trips['pickup_longitude'] <= -73) & \n",
    "    (df_trips['pickup_longitude'] > -75) & \n",
    "    (df_trips['pickup_latitude'] >= 40) & \n",
    "    (df_trips['pickup_latitude'] < 42) &\n",
    "    (df_trips['time_length'] > 0.02)\n",
    "]\n",
    "\n",
    "print(f\"Dataframe shape after cleaning: {df_trips_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def47278",
   "metadata": {},
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6afeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df_trips_filtered[['pickup_longitude', 'pickup_latitude','dropoff_longitude', 'dropoff_latitude']]\n",
    "labelDistance = df_trips_filtered['trip_distance']\n",
    "\n",
    "x_trainDistance, x_testDistance, y_trainDistance, y_testDistance = train_test_split(inputs, labelDistance, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e835237f",
   "metadata": {},
   "source": [
    "# Choice of regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d77dd1",
   "metadata": {},
   "source": [
    "## Comparison of possible regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ca55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, BayesianRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models for comparison\n",
    "models = [\n",
    "    ('Linear Regression', LinearRegression()),\n",
    "    ('Random Forest Regression', RandomForestRegressor()),\n",
    "    ('Decision Tree Regression', DecisionTreeRegressor()),\n",
    "    ('K-Nearest Neighbors Regression', KNeighborsRegressor()),\n",
    "    ('Gradient Boosting Regression', GradientBoostingRegressor()),\n",
    "    ('AdaBoost Regression', AdaBoostRegressor()),\n",
    "    ('Ridge Regression', Ridge()),\n",
    "    ('Lasso Regression', Lasso()),\n",
    "    ('Elastic Net Regression', ElasticNet()),\n",
    "    ('Bayesian Regression', BayesianRidge()),\n",
    "    ('Polynomial Regression', make_pipeline(PolynomialFeatures(degree=2), LinearRegression()))\n",
    "]\n",
    "\n",
    "# Loop through models and calculate MAE\n",
    "for name, model in models:\n",
    "    model.fit(x_trainDistance, y_trainDistance)\n",
    "    y_pred = model.predict(x_testDistance)\n",
    "    mae = mean_absolute_error(y_testDistance, y_pred)\n",
    "    print(f\"{name} MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ad219",
   "metadata": {},
   "source": [
    "## Comparison Result\n",
    "As can be seen from above, the RF regression model is the most accurate. This will therefore be the model that will be kept for the rest of this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff7d40",
   "metadata": {},
   "source": [
    "# Predictive model - distance cost matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed740f9",
   "metadata": {},
   "source": [
    "## Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa65c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train chosen model\n",
    "finModelDistance = RandomForestRegressor()\n",
    "finModelDistance.fit(x_trainDistance, y_trainDistance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82c99f",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecb5a7",
   "metadata": {},
   "source": [
    "### Importing depot and shop coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148577c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depot coordinates and DF\n",
    "depot_long = -73.941868\n",
    "depot_lat = 40.725516\n",
    "df_depot = pd.DataFrame({'id':'Depot', 'lat': [depot_lat], 'long': [depot_long]})\n",
    "\n",
    "# Shops' coordinates and DF\n",
    "df_shops = pd.read_csv('2015_shop_locations.csv')\n",
    "df_shops = df_shops.drop(columns=['demand(kg)', 'stage'])\n",
    "\n",
    "# Create new df that concatenates depot coordinates with df_shops\n",
    "df_points = pd.concat([df_depot, df_shops], ignore_index=True)\n",
    "df_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49848e",
   "metadata": {},
   "source": [
    "### Distance cost matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df129c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the cost matrix using every couple of ids in df_points\n",
    "## First create a new df for the cost matrix\n",
    "costMatrixDistance = pd.DataFrame(columns=df_points['id'], index=df_points['id'])\n",
    "\n",
    "## Second fill the cost matrix with the predicted values\n",
    "for id1 in df_points['id']:\n",
    "    # Get the latitude and longitude for id1\n",
    "    lat1 = df_points.loc[df_points['id'] == id1, 'lat'].values[0]\n",
    "    long1 = df_points.loc[df_points['id'] == id1, 'long'].values[0]\n",
    "    \n",
    "    for id2 in df_points['id']:\n",
    "        # Get the latitude and longitude for id2\n",
    "        lat2 = df_points.loc[df_points['id'] == id2, 'lat'].values[0]\n",
    "        long2 = df_points.loc[df_points['id'] == id2, 'long'].values[0]\n",
    "\n",
    "        costMatrixDistance.loc[id1, id2] = finModelDistance.predict([[long1, lat1, long2, lat2]])[0]\n",
    "\n",
    "costMatrixDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d575f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "costMatrixDistance.to_csv('costMatrixDistance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cff42e",
   "metadata": {},
   "source": [
    "# Predictive model - duration cost matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceff77f",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e065867",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTime = df_trips_filtered['time_length']\n",
    "\n",
    "x_trainTime, x_testTime, y_trainTime, y_testTime = train_test_split(inputs, labelTime, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e415d8b7",
   "metadata": {},
   "source": [
    "## Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train chosen model\n",
    "finModelDuration = RandomForestRegressor()\n",
    "finModelDuration.fit(x_trainTime, y_trainTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ddd2bf",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b9560f",
   "metadata": {},
   "source": [
    "### Importing depot and shop coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ddad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depot coordinates and DF\n",
    "depot_long = -73.941868\n",
    "depot_lat = 40.725516\n",
    "df_depot = pd.DataFrame({'id':'Depot', 'lat': [depot_lat], 'long': [depot_long]})\n",
    "\n",
    "# Shops' coordinates and DF\n",
    "df_shops = pd.read_csv('2015_shop_locations.csv')\n",
    "df_shops = df_shops.drop(columns=['demand(kg)', 'stage'])\n",
    "\n",
    "# Create new df that concatenates depot coordinates with df_shops\n",
    "df_points = pd.concat([df_depot, df_shops], ignore_index=True)\n",
    "df_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d454163",
   "metadata": {},
   "source": [
    "### Duration cost matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fa411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the cost matrix using every couple of ids in df_points\n",
    "## First create a new df for the cost matrix\n",
    "costMatrixDuration = pd.DataFrame(columns=df_points['id'], index=df_points['id'])\n",
    "\n",
    "## Second fill the cost matrix with the predicted values\n",
    "for id1 in df_points['id']:\n",
    "    # Get the latitude and longitude for id1\n",
    "    lat1 = df_points.loc[df_points['id'] == id1, 'lat'].values[0]\n",
    "    long1 = df_points.loc[df_points['id'] == id1, 'long'].values[0]\n",
    "    \n",
    "    for id2 in df_points['id']:\n",
    "        # Get the latitude and longitude for id2\n",
    "        lat2 = df_points.loc[df_points['id'] == id2, 'lat'].values[0]\n",
    "        long2 = df_points.loc[df_points['id'] == id2, 'long'].values[0]\n",
    "\n",
    "        costMatrixDuration.loc[id1, id2] = finModelDuration.predict([[long1, lat1, long2, lat2]])[0]\n",
    "\n",
    "costMatrixDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ceec153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "costMatrixDuration.to_csv('costMatrixDuration.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b203d24",
   "metadata": {},
   "source": [
    "# Predictive model - financial cost matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13161e81",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fed48e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelFinancial = df_trips_filtered['total_amount'] - df_trips_filtered['tip_amount']\n",
    "\n",
    "x_trainFinancial, x_testFinancial, y_trainFinancial, y_testFinancial = train_test_split(inputs, labelFinancial, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3eca1b",
   "metadata": {},
   "source": [
    "## Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train chosen model\n",
    "finModelFinancial = RandomForestRegressor()\n",
    "finModelFinancial.fit(x_trainFinancial, y_trainFinancial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d05a9d0",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480204b",
   "metadata": {},
   "source": [
    "### Importing depot and shop coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depot coordinates and DF\n",
    "depot_long = -73.941868\n",
    "depot_lat = 40.725516\n",
    "df_depot = pd.DataFrame({'id':'Depot', 'lat': [depot_lat], 'long': [depot_long]})\n",
    "\n",
    "# Shops' coordinates and DF\n",
    "df_shops = pd.read_csv('2015_shop_locations.csv')\n",
    "df_shops = df_shops.drop(columns=['demand(kg)', 'stage'])\n",
    "\n",
    "# Create new df that concatenates depot coordinates with df_shops\n",
    "df_points = pd.concat([df_depot, df_shops], ignore_index=True)\n",
    "df_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a850f2",
   "metadata": {},
   "source": [
    "### Financial cost matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87696752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the cost matrix using every couple of ids in df_points\n",
    "## First create a new df for the cost matrix\n",
    "costMatrixFinancial = pd.DataFrame(columns=df_points['id'], index=df_points['id'])\n",
    "\n",
    "## Second fill the cost matrix with the predicted values\n",
    "for id1 in df_points['id']:\n",
    "    # Get the latitude and longitude for id1\n",
    "    lat1 = df_points.loc[df_points['id'] == id1, 'lat'].values[0]\n",
    "    long1 = df_points.loc[df_points['id'] == id1, 'long'].values[0]\n",
    "    \n",
    "    for id2 in df_points['id']:\n",
    "        # Get the latitude and longitude for id2\n",
    "        lat2 = df_points.loc[df_points['id'] == id2, 'lat'].values[0]\n",
    "        long2 = df_points.loc[df_points['id'] == id2, 'long'].values[0]\n",
    "\n",
    "        costMatrixFinancial.loc[id1, id2] = finModelFinancial.predict([[long1, lat1, long2, lat2]])[0]\n",
    "\n",
    "costMatrixFinancial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05e8f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "costMatrixFinancial.to_csv('costMatrixFinancial.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
